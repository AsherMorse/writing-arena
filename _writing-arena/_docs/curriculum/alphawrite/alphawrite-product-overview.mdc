---
description: Overview of Alphawrite (aka Scribe)
globs: **/*
---
## **Summary**

Alphawrite is an AI-driven writing tutor to help students master the fundamentals of writing by writing real sentences and receiving instant feedback from AI.

It’s modeled off “The Writing Revolution” by Judith C. Hochman and Natalie Wexler, which offers a strategic approach to improving writing skills by emphasizing the foundational role of sentence-level proficiency. 

The book advocates for structured, explicit instruction in sentence construction, grammar, and the development of coherent paragraphs and essays. By integrating writing into content learning across subjects, it aims to enhance students’ critical thinking and comprehension capabilities.

Alphawrite is currently focused on grades 3-5, with the goal of supporting K-12.

## **High-level goals**

In Alphawrite, students:

* Write about what they love  
  * Example: Activities are created for each individual student, using their own interests  
* Learn by actually writing sentences  
  * Example: Activities are designed (where possible) to make students write sentences or paragraphs themselves  
* Master concepts before moving on  
  * Example: The curriculum is designed to encourage mastery  
* Practice at their current skill level  
  * Example: activities are tuned for a student’s abilities  
* Practice every day   
  * Example: students are encouraged to practice 10+ minutes a day; activities are designed to be completed in one session  
* Receive instant, helpful feedback   
  * Example: grading is performed by AI and identifies ALL mistakes  
  * Example: feedback is provided by AI on every submission

Overall, Alphawrite as an application should:

* Adhere to the principles of writing instruction defined in “The Writing Revolution”  
  * Example: Activities should be described in TWR, and be (roughly) presented in the order implied by TWR  
  * Exception: We currently don’t adhere to the suggestion that writing instruction should be performed throughout the teaching of other subjects, because we can’t influence the behavior of other learning apps.  
* Encourage students to come back and continue practicing  
  * Clarification: Motivation is the primary responsibility of Alpha School, and isn’t our explicit responsibility to include. However, we strive to build applications with great UX and Alphawrite should feel engaging and encouraging to students; it should be fun to use and remove friction when practicing.  
* Provide appropriate APIs to allow integration with other applications  
  * Example: allow a student’s next Activity to be queried via API so it can be integrated into Alpha School’s dashboard  
* Provide accessible analytics to allow student guides to evaluate an individual student’s progress  
  * Example: identify that a student is struggling and move them back down a grade or difficulty level

From a learning outcomes perspective, Alphawrite aims to:

* For all students:  
  * Meet or exceed Common Core standard requirements on standardized tests at their age grade level  
  * Achieve growth of two grade levels in one calendar year (a general goal across subjects)  
* For high school students:  
  * Prepare students to get high scores (4s and 5s) on AP tests  
  * Prepare students for college admission essays

While not explicit requirements, the application may:

* Take Common Core requirements directly into account when creating content for students  
* Incorporate temporally relevant content from other subjects, if that data is accessible

## **High-level student experience**

Students perform “Activities”, which are bite-sized exercises focused on a particular writing skill. One example is “Unscramble Sentences”, in which a student receives a scrambled sentence and has to unscramble it by rewriting the sentence with the words in the right order and with proper capitalization and punctuation. 

There are \~25 activities currently, starting with the basic building blocks of sentences and moving up through paragraphs. We don’t yet help students write entire essays.

Every student is assigned to a particular grade level. Within each grade level, there are “Easy”, “Medium” and “Hard” difficulty levels for each Activity. At the beginning of a school year, students are set to the correct grade level, then start performing Activities at the “Easy” difficulty. They go through every activity on “Easy” before starting over and performing all of the activities on “Medium”, then on “Hard”. The idea here is that this allows Alphawrite to reinforce concepts gradually over time.

# How it works

## **How Activity content is created**

All of the content in the application is generated by AI. There are three main objectives when generating content, some of which have sub-objectives:

1. **Difficulty:** Match a student’s unique level of knowledge and understanding.  
   1. **Lexile Level:** What’s their level of reading comprehension skill?  
      1. **Example:** can the student understand a particular sentence?  
         1. We use the Lexile Level measurement to estimate this.  
            1. “The dog saw the ball” has a lower lexile number than “Four score and seven years ago our fathers brought forth, upon this continent, a new nation…”  
      2. **Status:** Completed and working  
   2. **Prior Knowledge**: What other things does a student know that might be mentioned in a given sentence?  
      1. **Example:** a writing exercise that mentions the Declaration of Independence probably requires students to have already learned about the Declaration of Independence in History class.  
      2. **Status:** Not yet created. This requires someone (either us, or a different product) to maintain some sort of index about what a student knows by aggregating their learning across applications. It’s quite complex to create and isn’t yet working.  
         1. **Our current solution:** In the meantime, we try to guess what a student knows by assuming they are keeping up with the Core Knowledge curriculum, which defines a set of topics that students at a particular grade level should know.  
            1. We have extracted the entire Core Knowledge curriculum from relevant sources and placed it in a structured format that can be used in applications.  
   3. **Comprehension Speed:** How quickly can a student answer the question given how much content is displayed on the screen?  
      1. **Example:** in “Unscramble Sentences”, longer sentences are harder to unscramble, so less experienced students receive shorter sentences.   
      2. **Status:** Completed and working.  
      3. **Note:** The way we adjust the amount of content to meet the student’s Comprehension Speed varies based on the type of activity, since the structure of the content in each activity is unique.  
2. **Topical Relevance**: Let students write about something they love by making the content about their unique, individualized interests.  
   1. **Example:** a young student likes baseball and is currently playing for a little league team. We might create them a custom activity using   
   2. **Status:** Not yet created.  
      1. **Our current solution:** We created a list of topics matching the Core Knowledge curriculum that we thought students would like.  
3. **Adherence to Curriculum Standards:** Ensure that the content displayed to students covers the standards defined in Common Core.  
   1. **Status**: Only implemented for a small subset of activities, and we don’t track adherence to Common Core.  
      1. **Our current solution:** This was a requirement initially introduced, but has since been pushed to the backburner because of its complexity.   
         1. The key problem here is that attempting to generate content matching specific Common Core standards is complex and requires lots of iterative generation cycles to get appropriate content; when you mix this with attempting to generate content specific for a particular student, the generation problem can quickly become over-constrained as you’re asking the large language model to optimize for too many goals simultaneously (and typically the LLM doesn’t know how to effectively make tradeoffs between all of its various instructions in a way that generates something effective for the student).

\[\*\] **I want to add emphasis to the previous section:** Right now, all of the content is pre-generated by AI to match only static difficulty levels, like Lexile. We also can’t generate content for specific students yet. 

## **High-level of grading and feedback**

In Alphawrite, **grading** refers to the process of taking the student’s answer, comparing it against a **rubric** which defines what a correct answer must contain, and ultimately deciding if the answer is **correct** or **incorrect**.

In the case that the answer is **incorrect**, we provide the errors identified during grading to a specialized AI system which turns them into a written piece of **feedback** for the student. 

Feedback is streamed live to the student’s browser right after they answer a question, keeping the feedback loop tight.

## **How grading works**

We’ve developed a bank of **Criteria** which can be added to a Rubric. Each Criteria aims to evaluate a student’s written answer on one particular dimension. 

Some examples of criteria we use:

* **Deterministic checks:** Use simple string operations, regex matching, or other synchronous techniques to look for features in a sentence.  
  * **Example:** Ensure a sentence ends with a period  
* **NLP checks:** Use traditional natural language processing libraries to look for features in a sentence.  
  * **Example:** Find all proper nouns and ensure that they’re all capitalized.  
* **AI checks:** Call large language models to evaluate written sentences.  
  * **Example:** Ensure that a sentence is semantically correct; “The dog ate my homework” makes sense, but “The homework ate my dog” does not.  
  * **Example:** Identify the “type” of a sentence (declarative, imperative, interrogative, exclamatory) and make sure it matches an expected type.  
    * This is actually a hybrid check, because interrogative and exclamatory can be performed deterministically, while the others cannot.  
* **External checks:** We use certain third-party providers to perform evaluations of a sentence.  
  * **Example:** We use LanguageTool which provides a whole host of grammatical checks on a particular sentence. LanguageTool excels at rule-based grammar issues, or identification of typos, but fails to identify semantic issues which might be relevant in a given activity, so we need to use it in combination with AI.

The Criteria are compiled into a Rubric, which orchestrates the execution of each individual check and compiles all of the results into a list.

## **How feedback works**

If the grading process deems an answer to be incorrect, we gather up the incorrect items from the Rubric, transform them into formatted snippets of text, and input these into AI to write feedback for the student.

The instructions provided to AI are unique to each individual Activity, because we need to take into account what the student was asked to do, what their answer was, and how the AI should choose to provide feedback given their mistakes.

We track all grading rubrics and feedback every time students submit something, so we can later inspect these for quality or produce other content on their basis, for example writing an update email to the student or to the student’s parents describing their most common mistakes.